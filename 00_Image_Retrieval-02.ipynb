{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gDSGwFsAJk3"
      },
      "source": [
        "O fluxo do programa é  \n",
        "\n",
        "1) baixar 1433 arquivos TAR públicos com os dados e as imagens das patentes publicadas entre 2001 e 2021 diretamente do repositório de patentes dos Estados Unidos conhecido como Bulk Data Storage System (BDSS) no diretório `imgs/raw` criado por nós em nosso Drive do Google e  \n",
        "\n",
        "2) descompactar esses arquivos TAR no mesmo diretório `imgs/raw`. Ao descompactar esses arquivos temos uma estrutura de diretórios no formato `IYYYYMMDD` e em cada diretório vários subdiretórios no formato `UTIL99999` (entre outros). Em cada subdiretório desses encontram-se vários arquivos no formato `USXX999999-YYYYMMDD.ZIP` que somados chegam a cerca de 4 milhões de arquivos.  \n",
        "\n",
        "3) Descompactamos esses arquivos ZIP para o diretório `imgs/lake`. Ao descompactá-los temos um diretório para cada patente no formato `USXX999999-YYYYMMDD`. Em cada diretório, encontram-se um arquivo `.XML` com as informações textuais da patente e imagens das patentes no formato `TIFF` que podem variar de 1 a algumas centenas de imagens. (TODO: calcular a média de imagens por patente).  \n",
        "\n",
        "4) Lemos o arquivo `.XML` e obtemos a classe IPC da patente. Então, convertemos as imagens presentes de TIFF para JPEG (ou seria PNG melhor?) e as enviamos para o diretório correspondente à classe IPC da patente, i.e. `imgs/ipc/X99`. No total temos 131 diferentes diretórios. Um para cada classe IPC. (Deveríamos aprofundar mais e usar as 647 subclasses IPC? O que isso traria de vantagem?)  \n",
        "\n",
        "Agora temos as imagens convertidas e divididas em suas respectivas classes, prontas para serem manipuladas pelo algoritmo de extração de características (features extraction).  \n",
        "\n",
        "5) Onde e como guardar essas features? Em um banco de dados? Como?  \n",
        ".  \n",
        ".  \n",
        "**A estrutura de diretórios / arquivos é mais ou menos a seguinte:**  \n",
        "\n",
        "---\n",
        "`imgs/raw/`  \n",
        "`---- I20210112.tar (file)(untar here)`  \n",
        "\n",
        "`---- I20210112 (dir)`  \n",
        "`-------- UTIL10891 (dir)`  \n",
        "`------------ US108911000-20210112.ZIP (unzip to imgs/lake)`  \n",
        "`------------ US108911001-20210112.ZIP`  \n",
        "`------------ US108911002-20210112.ZIP`  \n",
        "`------------ etc`  \n",
        "`-------- UTIL10892 (dir)`  \n",
        "`-------- UTIL10893 (dir)`  \n",
        "`-------- etc.`  \n",
        "\n",
        "---\n",
        "`imgs/lake`  \n",
        "`----US108911000-20210112 (dir)`  \n",
        "`--------US108911000-20210112.XML (file)`  \n",
        "`--------US108911000-20210112-D00000.TIF (file)`  \n",
        "`--------US108911000-20210112-D00001.TIF (file)`  \n",
        "`--------US108911000-20210112-D00002.TIF (file)`  \n",
        "`--------etc.`  \n",
        "Read XML, get IPC class(es) convert from TIFF to JPEG and send it to IPC folder.  \n",
        "\n",
        "---\n",
        "`imgs/ipc`  \n",
        "`---- A01`  \n",
        "`-------- US108911000-20210112-D00000.JPG`  \n",
        "`-------- US108911000-20210112-D00001.JPG`  \n",
        "`-------- etc.`  \n",
        "`---- A02`  \n",
        "`---- A03`    \n",
        "`etc.`  \n",
        "\n",
        "---\n",
        "\n",
        "Mais informações encontram-se ao longo do código.  \n",
        "\n",
        "**Passos realizados até agora:**\n",
        "1. <strike>criar diretórios para as classes IPC</strike>\n",
        "2. <strike>listar e salvar links com arquivos TAR</strike>\n",
        "3. <strike>baixar arquivos TAR</strike>\n",
        "4. <strike>listar e descompactar arquivos TAR</strike>\n",
        "5. <strike>listar subarquivos ZIP</strike>\n",
        "6. <strike>descompactar subarquivos ZIP para um diretório comum</strike>\n",
        "7. <strike>apagar tudo que não for mais necessário para economizar espaço.</strike> Ainda mantemos os arquivos TAR no Drive. Estamos apagando apenas os subarquivos ZIP por enquanto.\n",
        "\n",
        "Os seguintes passos encontram-se em outro notebook:\n",
        "8. ler full text, determinar classe IPC da patente, transformar e enviar imagem para o respectivo diretório específico"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa-ORCdQxbDs"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###0. setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conectamo-nos ao drive, onde salvamos os dados provenientes do Bulk Data Storage System (BDSS)."
      ],
      "metadata": {
        "id": "C26ndHehqCjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtSRWta3ny5u",
        "outputId": "edb8eb6b-36d2-478d-f01f-987c40b21934"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalamos o wget para poder baixar os dados.  \n",
        "TODO: testar um outro módulo que já esteja instalado no colab para fazer o download dos dados."
      ],
      "metadata": {
        "id": "OqrKEmVkqsUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qdkbu0oKhwdC"
      },
      "outputs": [],
      "source": [
        "#install uninstalled modules\n",
        "!pip install wget"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos as bibliotecas e módulos necessários para o funcionamento do código."
      ],
      "metadata": {
        "id": "ug_KVzkIrKry"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hZ4K1bP-fQoD"
      },
      "outputs": [],
      "source": [
        "#import libraries and modules\n",
        "import os\n",
        "import pickle\n",
        "#TODO: test other module than wget\n",
        "#import wget\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import requests\n",
        "\n",
        "import tarfile\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Declaramos algumas constantes que apontam para alguns diretórios em nosso disco.  \n",
        "\n",
        "**`DATA_DIR`** é onde salvamos listas de paths, features, etc. em formato pickle.  \n",
        "\n",
        "**`RAW_IM`** é onde salvamos e descompactamos os arquivos TAR provenientes diretamente do Bulk Data Storage System (BDSS).\n",
        "\n",
        "**`IPC_IM`** é o diretório onde criamos a estrutura de subdiretórios baseados nas classes IPC. Neles salvamos as imagens convertidas de TIFF para JPG (ou seria melhor PNG?). Os algoritmos de extração de features lêem esses subdiretórios e buscam aqui as imagens JPG (/PNG).\n",
        "\n",
        "**`IM_LAKE`** é onde descompactamos os subarquivos ZIP que contêm diretórios para cada patente com os arquivos XML e as imagens em formato TIFF."
      ],
      "metadata": {
        "id": "mR4K2bQkrW-7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gSReUfmbOZeW"
      },
      "outputs": [],
      "source": [
        "#important paths\n",
        "\n",
        "DATA_DIR = '/content/drive/My Drive/octimine/data/' \n",
        "IM_LAKE = '/content/drive/My Drive/octimine/imgs/lake/' \n",
        "IPC_IM = '/content/drive/My Drive/octimine/imgs/ipc/'\n",
        "RAW_IM = '/content/drive/My Drive/octimine/imgs/raw/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgkRiEML1paO"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###1. create IPC classes folders"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista com todas as 131 classes IPC.  \n",
        "Será que deveríamos usar as 647 subclasses IPC? Isso traria algum benefício na hora de extrair as features?  \n",
        "Mais informações: https://www.wipo.int/"
      ],
      "metadata": {
        "id": "TCG5F9iCEJqp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57G2r-kcemR6"
      },
      "outputs": [],
      "source": [
        "#create folders related to IPC classes\n",
        "#https://ipcpub.wipo.int/\n",
        "\n",
        "ipc_classes = [\n",
        "  'A01','A21','A22','A23','A24','A41','A42','A43','A44','A45','A46','A47','A61','A62','A63','A99',\n",
        "  'B01','B02','B03','B04','B05','B06','B07','B08','B09','B21','B22','B23','B24','B25','B26','B27','B28','B29','B30','B31','B32','B33','B41','B42','B43','B44','B60','B61','B62','B63','B64','B65','B66','B67','B68','B81','B82','B99',\n",
        "  'C01','C02','C03','C04','C05','C06','C07','C08','C09','C10','C11','C12','C13','C14','C21','C22','C23','C25','C30','C40','C99',\n",
        "  'D01','D02','D03','D04','D05','D06','D07','D21','D99',\n",
        "  'E01','E02','E03','E04','E05','E06','E21','E99',\n",
        "  'F01','F02','F03','F04','F15','F16','F17','F21','F22','F23','F24','F25','F26','F27','F28','F41','F42','F99',\n",
        "  'G01','G02','G03','G04','G05','G06','G07','G08','G09','G10','G11','G12','G16','G21','G99',\n",
        "  'H01','H02','H03','H04','H05','H99'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cria os diretórios baseando-se na lista acima, utilizando-se a biblioteca `os`.  \n",
        "\n",
        "Lê a lista.\n",
        "Para cada ítem da lista, cria-se um path IPC_IM + ítem.\n",
        "Verifica se já existe o diretório.  \n",
        "Caso afirmativo, volta-se para o início do loop.  \n",
        "Caso negativo, criar-se o diretório usando o path."
      ],
      "metadata": {
        "id": "MFBFP_3tE3Kh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXG3fEUFbFqt"
      },
      "outputs": [],
      "source": [
        "#mkdir\n",
        "for ipc_class in ipc_classes:\n",
        "  ipc_path = os.path.join(IPC_IM, ipc_class)\n",
        "  ipc_exists = os.path.isdir(ipc_path)\n",
        "\n",
        "  if ipc_exists:\n",
        "    print(f'Folder {ipc_class} exists already! Moving on...')\n",
        "  else:\n",
        "    os.mkdir(ipc_path)\n",
        "    print(f'Folder {ipc_path} successfully created.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTW_M8p41xjS"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###2. list paths of TAR files to be downloaded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lista todos os anos a serem pesquisados.  \n",
        "Cria-se uma lista vazia que armazenará os endereços dos arquivos a serem baixados."
      ],
      "metadata": {
        "id": "fiEmqEsPF0mm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ag_JLbucfS6F"
      },
      "outputs": [],
      "source": [
        "#list TAR files paths\n",
        "#https://www.crummy.com/software/BeautifulSoup/bs4/doc.ptbr/ \n",
        "\n",
        "tar_files = []\n",
        "\n",
        "#from 2001 to 2010 the files are ZIP\n",
        "#from 2011 onwards the files are TAR\n",
        "\n",
        "years = [\n",
        "  '2021',\n",
        "  '2020', '2019', '2018', '2017', '2016', '2015', '2014', '2013', '2012', '2011',\n",
        "  '2010', '2009', '2008', '2007', '2006', '2005', '2004', '2003', '2002', '2001'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Popula a lista tar_files.  \n",
        "\n",
        "Lê a lista years e determina-se a url a ser pesquisada.  \n",
        "Utilizando-se a biblioteca BeautifulSoup analisa-se (parse) o documento .html e extraem-se os links que contêm a palavra-chave (key) que é o ano em questão.  \n",
        "Caso algo seja encontrado, é adicionado à lista tar_files.\n",
        "Imprime-se o número de ítens na lista."
      ],
      "metadata": {
        "id": "IoZH9TZ_GPlo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yat3z5MdpgRZ",
        "outputId": "b29e8652-f95f-4c11-9b89-3a0d44bcf37f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1433\n"
          ]
        }
      ],
      "source": [
        "for year in years:\n",
        "  url = f'https://bulkdata.uspto.gov/data/patent/grant/redbook/{year}/'\n",
        "  soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n",
        "\n",
        "  #https://www.crummy.com/software/BeautifulSoup/bs4/doc.ptbr/#os-argumentos-palavras-chave\n",
        "  #keyword, common to all files to be downloaded\n",
        "  keyword = year\n",
        "\n",
        "  for x in soup.find_all(href=re.compile(keyword)):\n",
        "      tar_files.append(url + x.get('href'))\n",
        "\n",
        "print(len(tar_files))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva-se a lista `tar_files` para uso posterior caso se dê prosseguimento com a operação apenas mais tarde."
      ],
      "metadata": {
        "id": "GXhr_b4lHtOV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k9znLIv3g4p3"
      },
      "outputs": [],
      "source": [
        "#save TAR files list for later use\n",
        "pickle.dump(\n",
        "    tar_files,\n",
        "    open(DATA_DIR + 'TAR-files.pickle', 'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_MR-lA11Vd"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###3. download TAR files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbyVIkevvz4D"
      },
      "outputs": [],
      "source": [
        "#open TAR files list\n",
        "tar_files = pickle.load(open(DATA_DIR + 'TAR-files.pickle', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9A0jxbBXEXT"
      },
      "outputs": [],
      "source": [
        "#download TAR files\n",
        "for tar_file in tar_files:\n",
        "  file_name = tar_file[-12:]\n",
        "  file_path = RAW_IM + file_name\n",
        "  file_exists = os.path.isfile(file_path)\n",
        "\n",
        "  if file_exists:\n",
        "    print(f'File {file_name} has been downloaded already! Moving on...')\n",
        "  else:\n",
        "    wget.download(tar_file, file_path)\n",
        "    print(f'Downloading file {file_name}. Please wait...')    \n",
        "    print(f'File {file_name} successfully downloaded.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O10jUNIFIfHf"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###4. list, untar and delete downloaded TAR source files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDOOtvj0kxVS",
        "outputId": "2f7cc32b-105c-402a-8a2e-76c5a4f60346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "108"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#list tar files\n",
        "untar_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(RAW_IM):\n",
        "  for x in files:\n",
        "    #check only for .tar files\n",
        "    if x.endswith('.tar'):\n",
        "      untar_files.append(os.path.join(root, x))\n",
        "  #break to list only first level folder\n",
        "  #https://stackoverflow.com/a/20868760/3499881\n",
        "  break\n",
        "\n",
        "len(untar_files)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salva-se a lista `untar_files` para uso posterior caso se dê prosseguimento com a operação apenas mais tarde."
      ],
      "metadata": {
        "id": "xbzd1el3IpHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save unTAR files list for later use\n",
        "pickle.dump(\n",
        "    untar_files,\n",
        "    open(DATA_DIR + 'unTAR-files.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "RcXv0uA3bnge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "####4.2 untar and delete"
      ],
      "metadata": {
        "id": "9h0O54IQb5UN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#open unTAR files list\n",
        "untar_files = pickle.load(open(DATA_DIR + 'unTAR-files.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "7z3Wm-CTb0TC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTfpb_cUyAE7"
      },
      "outputs": [],
      "source": [
        "#untar files\n",
        "for untar_file in untar_files:\n",
        "  folder_name = 'I' + untar_file[-12:-4]\n",
        "  folder_path = RAW_IM + folder_name\n",
        "  folder_exists = os.path.isdir(folder_path)\n",
        "\n",
        "  if folder_exists:\n",
        "    print(f'Folder {folder_name} exists already! Moving on...')\n",
        "\n",
        "  else:\n",
        "    print(f'Untaring file {untar_file}. Please wait...')    \n",
        "    untar = tarfile.open(untar_file)\n",
        "    untar.extractall(RAW_IM)\n",
        "    untar.close()\n",
        "    print(f'File {untar_file} successfully untared.')\n",
        "\n",
        "    #delete original tar files to save space\n",
        "    #print(f'Deleting {untar_file}. Please wait...')\n",
        "    #os.remove(untar_file)\n",
        "    #print(f'File {untar_file} successfully deleted.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0tA4TNM2BSf"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "###5. list, unzip and delete ZIP (sub)files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSGymOuOvR9S",
        "outputId": "0b44fbf6-6bc4-414c-d38d-327d7fda7769"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391056"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "#list ZIP subfiles\n",
        "zip_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(RAW_IM):\n",
        "  for x in files:\n",
        "    if x.endswith('.ZIP'):\n",
        "      zip_files.append(os.path.join(root, x))\n",
        "\n",
        "len(zip_files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save ZIP files list for later use\n",
        "pickle.dump(\n",
        "    zip_files,\n",
        "    open(DATA_DIR + 'ZIP-files.pickle', 'wb'))"
      ],
      "metadata": {
        "id": "L6hHUk2JYgHV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "####5.2 unzip and delete"
      ],
      "metadata": {
        "id": "BZziT7dncNnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#open ZIP files list\n",
        "zip_files = pickle.load(open(DATA_DIR + 'ZIP-files.pickle', 'rb'))"
      ],
      "metadata": {
        "id": "SfabEw_PYsNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYMoqag-0W3u"
      },
      "outputs": [],
      "source": [
        "#unzip ZIP subfiles\n",
        "#TODO: verificar se tarfile descompactaria arquivos zip e vice-versa\n",
        "for zip_file in zip_files:\n",
        "  folder_name = zip_file[-23:-4]\n",
        "  supp_name = zip_file[-28:-4]\n",
        "  folder_path = IM_LAKE + folder_name\n",
        "  folder_supp = IM_LAKE + supp_name\n",
        "  folder_exists = os.path.isdir(folder_path)\n",
        "  supp_exists = os.path.isdir(folder_supp)\n",
        "\n",
        "  if folder_exists or supp_exists:\n",
        "    print(f'Folder {folder_name} exists already! Moving on...')\n",
        "    #delete original zip subfile\n",
        "    print(f'Deleting {zip_file}. Please wait...')\n",
        "    os.remove(zip_file)\n",
        "    print(f'File {zip_file} successfully deleted.')\n",
        "\n",
        "  else:\n",
        "    print(f'1. Unzipping file {zip_file}. Please wait...')\n",
        "    ZipFile(zip_file, 'r').extractall(path=IM_LAKE)\n",
        "    print(f'2. File {zip_file} successfully unzipped.')\n",
        "    #delete original zip subfile\n",
        "    print(f'3. Deleting {zip_file}. Please wait...')\n",
        "    os.remove(zip_file)\n",
        "    print(f'4. File {zip_file} successfully deleted.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsITiy1HayPsJGBqiWf0rA"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}